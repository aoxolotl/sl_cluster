# Unsupervised Clustering for Sign Languages

Despite the progress in natural language processing of spoken languages, deep
learning research for sign languages is still in a very nascent stage.  Sign
languages occupy a different modality yet have the same characteristics as
spoken languages such as grammar, syntax and phonology. With very limited very
limited availability of sign language transcriptions, it becomes important to
explore unsupervised and weakly supervised techniques that are not reliant on
annotated data. Zero-resource language research for segmentation and term
discovery in speech has recently started gaining traction and we employ similar
methods on the RWTH dataset and analyze their efficacy for sign languages. More
importantly by treating sign language utterances no different from speech, we
hope to establish ready transferability of research from spoken languages.

Dataset used: [RWTH Phoenix 14T](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX/)

## Examples
![good cluster 1/1](./clusters/251/gifs/01June_2010_Tuesday_tagesschau.avi.gif)
![good cluster 1/2](./clusters/251/gifs/01June_2011_Wednesday_heute.avi.gif)
![good cluster 5/1](./clusters/251/gifs/02November_2010_Tuesday_heute.avi.gif)


![good cluster 5/1](./clusters/267/gifs/01May_2010_Saturday_tagesschau.avi.gif)
![good cluster 5/2](./clusters/267/gifs/01September_2010_Wednesday.gif)


![good cluster 5/1](./clusters/25/gifs/01May_2010_Saturday_tagesschau.avi.gif)
![good cluster 5/2](./clusters/25/gifs/02August_2010_Monday_heute.avi.gif)
